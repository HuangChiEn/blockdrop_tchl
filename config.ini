## Global paramters 
# Execution stage : str in options [train, finetune, test]
exe_stage = 'train'

# Wandb exp group id : str (optional)
exp_id = '003'

#----------------------------------------------------------------
## Common setup (model, datasets, trainer)
[model_params]  
    # we use timm, plz search tag by timm.list_models(.) 
    model_tag = 'resnet101'
    agent_tag = 'resnet18'

[dataset_params]
    dataset = 'imgnet'
    tra_ds_root = '/data1/dataset/imagenet_1k/train'
    tst_ds_root = '/data1/dataset/imagenet_1k/val'
    val_sub_ratio = 0.3
    num_workers = 8
    #  global_batch = $trainer.accumulate_grad_batches * batch_size * len(devices)
    batch_size = 256  

[trainer]
    # debug for tra/val loop via fast run!
    fast_dev_run = False
    accelerator = 'gpu'
    precision = 'bf16'
    devices = [1]
    accumulate_grad_batches = 8
    max_epochs = 45
    log_every_n_steps = 1

[logger]
    log_type = 'wandb'
    log_it = True
    [logger.setup]
        project = 'budget_dyn_nn'
        offline = True

#----------------------------------------------------------------
## Parameters setup for the specific procedure
[train_params]
    # mainly follows blockdrop github, instead of the paper!
    # https://github.com/Tushar-N/blockdrop?tab=readme-ov-file#curriculum-learning
    lr = 1e-3
    beta = 1e-1
    beg_cl_step = 1
    penalty = -1.0
    bound_alpha = 0.8

#----------------------------------------------------------------
[finetune_params]
    # https://github.com/Tushar-N/blockdrop?tab=readme-ov-file#joint-finetuning
    PATH = '/content/proj/budget_dyn_nn/bkpcxgwv/checkpoints/epoch=44-step=1035.ckpt'
    lr = 1e-4
    penalty = -5
    bound_alpha = 0.8

#----------------------------------------------------------------
# https://github.com/Tushar-N/blockdrop?tab=readme-ov-file#testing-and-profiling
[test_params]
    PATH = $finetune_params.PATH

[test_args]
    #benchmark = True
    #cal_flops = False
